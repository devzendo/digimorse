
Overall flow
------------
Transmitter:
Keyer -> [Keying Events] -> Source Encoder -> [Encoded Blocks] -> CRC Append -> [ Block Bytes ] -> Channel Encoder -> Costas Array Surround -> Modulator
                   |         ^                                                                                                                      ^
                   v        /                                                                                                                       |
                  CQ Detector                                                                                                                 UI TX Offset

Q. Does the Source Encoder add the CRC, or is it a separate step?

Receiver:
Audio In -> Fourier Transform -> [Spectrum] -> Costas Array Detector -> Channel Detectors

The Costas Array Detector finds Costas Arrays in the Spectrum stream, and allocates a Channel Detector at the Audio
Offset it finds the array. Each Channel Detector receives the Spectrum stream, which it passes through a Narrow Band
Pass Filter to see if it has any data - if so it demodulates it, etc...

Channel Detectors time out and are destroyed after 1 minute (configurable) of not receiving anything. This removes the
metadata they have received from the Waterfall display.


Channel Detector: [Spectrum, Audio Offset of Costas Array] -> Narrow Band Pass Filter -> Demodulator -> Channel Decoder -> [Source Encoded Block, Erasures?] -> CRC Filter ->
  -> Source Decoder -> [Metadata, Audio Offset, Keying Events] -> Receiver UI

Receiver UI: [Metadata, Audio Offset, Keying Events] -> Current User Band Pass Filter -> Merged Sound Output -> Audio Out
                                 |
                                 |
                                 v
                     Waterfall metadata display


Source Decoders are configured with their audio offset, and are given incoming, checksum-correct blocks of metadata and
keying events upon correct decoding by the channel decoder. Each channel detector can be marked by the UI as being in or
out of the overall bandpass filter. All channel detectors that are in the filter bandpass use their incoming keying
frames to reconstruct their keying sequence, which is used in a real-time-loop to control their amplitude. The audio
callback will iterate over all in-bandpass channel detectors' tone generators with the following algorithm:

// Add together all the active waveforms.
let sample = 0;
for generator in in_bandpass_set:
  let this_sample = generator.sinewave[generator.waveindex];
  // generator.sinewave lookup table constructed for each generator when
  // it is instantiated. the frequency of the lookup table is based on the audio offset of the detected signal.
  generator.waveindex ++;
  generator.waveindex %= generator.sinewave.length() // loop round the sinewave

  sample += this_sample * generator.amplitude // generator.amplitude is varied by processing the reconstructed keying sequence
sample /= in_bandpass_set.length();
output sample to buffer

There will always be at least one generator in the in_bandpass_set: the sidetone generator for the keyer. This is
different to channel decoders, as it receives a real-time stream of keying events from the keyer. Its amplitudes is
adjusted now, in real-time - no reconstruction required.


The reconstruction of the real-time keying for each channel decoder could be done by inserting
[decoder index, actual time of event, event type]...[..] into a priority heap, and pulling from this with reads that block until the
event occurs. e.g. given the following events for decoder index 0 (sidetone generator):

[0, now, START]
[0, 1000, UP] // dah
[0, 330, DOWN] // _
[0, 330, UP] // dit
[0, 330, DOWN] // _
[0, 1000, UP] // dah
[0, 2000, END] // maybe we get this instead of the last UP?
We would enqueue amplitude changes to be scheduled at:
(now, 1000, 1330, 1660, 1990, 2990, 4990?)
We decode the whole of the above block in one go, so can reconstruct easily.
What about keying that straddles multiple blocks? We have to filter out metadata and assume the keying continues
directly.
If a block fails to decode, we have to assume the end of keying; subsequent blocks start with sufficient information
to inform the generator what to do.

Any channel detectors that are removed due to timeout would have all entries for their decoder-index removed from the
priority heap.

The above sampling callback algorithm could also be responsible for adjusting the amplitude; the reconstruction
playback scheduler would set a variable in each generator to indicate the change required in the amplitude:
RISING, FALLING, STEADY. When RISING, the generator.amplitude is increased by some increment that generates a smooth
output waveform without key clicks. When it reaches its maximum, the variable would be set to STEADY. Similarly
when the scheduler sets it to FALLING, the generator amplitude will be decreased to minimum at which point the
callback sets it to STEADY.



Source Encoder Frames and Block Size
------------------------------------
The source encoder receives a stream of keying data, and must encode this into a compressed stream of encoded frames,
which are concatenated to form a block, which has a fixed size. If an incoming encoded frame would overflow the size of
the current block, the block is instead padded out to its maximum size, and emitted. The incoming encoded frame then
becomes the first frame of the new block. (It is pushed onto a stack of frames to emit)

Not holding back a block if the user pauses keying
--------------------------------------------------

If the stream of keying events 'dries up' without a keying end event, the block should be padded and emitted. This
would also happen on receipt of a keying end event.

Adding metadata into the source encoder frame stream
----------------------------------------------------
The keying stream would be split and connected to:
* a CQ detector (whose output is connected to the source encoder)
* the source encoder

The source encoder receives the keying stream, but when CQ is detected, it resets a state machine that includes the
callsign metadata as a source encoder frame in the current block (by pushing it onto the stack of frames to emit). [this
stack mechanism might cause the next metadata frame, the location, to be added to the same frame as the callsign]. the
state machine will queue up the three metadata frames (callsign, locator, power) to be encoded in the next three blocks.
If CQ is asserted when the state machine is not idle, it is ignored - if the user sends many CQs that overflow a block
then the first block emitted would contain the callsign and some CQ keying; the second block would include the location
and some CQ keying; the third block would include the power and some CQ keying. If they're still CQing when the state
machine goes idle, the fourth block could include the callsign again.

The source encoder also has a timer that queues the callsign metadata periodically even if there is no CQ detected; this
is to comply with license regulations that state stations must periodically identify themselves. This does not trigger
the three frames, just the callsign; a block will only contain one instance of the three frames: it could not contain
two callsigns for example.

If the user isn't keying, but leaves the station alone for a while, should this timer automatically generate a single
callsign metadata frame, padded to the block size, and emit it for channel encoding and transmission? Or should this
only happen during active operation?

Additional metadata frames
--------------------------
Every block contains a WPM and start-of-keying polarity frame, emitted prior to the first keying event.

Cyclic Redundancy Check
-----------------------
Upon emitting a block, a suitable CRC is appended. The strength of this would be chosen to be efficient but capable of
detecting a decent proportion of decode errors. What is 'good enough'? What are the trade-offs? To be investigated.

Source Encoder Frame Types
--------------------------
0. WPM/Polarity - add on first keying event of a new block.
1. Callsign
2. Location
3. Power
4. Keying - is start / stop needed?
5. Padding
6. (unused A)
7. Extension
This suggests 3 bits to encode the frame type would be sufficient. If the all-ones Extension type is found, this would
then be followed by a further 3 bits - which could also be an Extension. Leading to:
Second frame types:
0. (unused B)
1. (unused C)
2. (unused D)
3. (unused E)
4. (unused F)
5. (unused G)
6. (unused H)
7. Extension 2
Third frame types:
0. (unused I)
1. (unused J)
2. (unused K)
3. (unused L)
4. (unused M)
5. (unused N)
6. (unused O)
7. Extension 3
etc., etc. We'd have to cut this off sometime.... could be a denial of service / abused? And how would it work from a
forwards-compatibility perspective, as each frame type has a fixed length; how would an old client know how long a
received frame with an unused frame tag would be? Unless we also encode the frame length, or define an end of frame
marker or escaping mechanism


Keying Frames
-------------
Timing of each dit/dah/gap is given in milliseconds, as sent via the keyer. What range of ms might we see if we allow
the range 5-40WPM (cf Yaesu transceivers allow 5-60WPM). Perhaps I should increase the upper bound? (Where did I pick
40WPM from?)

http://www.k4icy.com/cw.html gives an overview of timing, incl. PARIS as the standard 'word' of 50 units in length that
can be divided into one minute to give the WPM rate. Other lengths: dit=1 unit, dah=3 units, pause between elements=1
unit, pause between letters=3 units, pause between words=7 units. PARIS includes a pause between words at the end.

So at 20WPM, that's 20x50=1,000 units in 1 minute. 1 minute = 60,000ms. So 60,000/1,000 = 60ms for each unit.
See Appendix A of "The Digimorse Communications Protocol".
