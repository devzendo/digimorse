
 Overall flow
 ------------
 Transmitter:
 Keyer -> [Keying Events] -> Source Encoder -> [Encoded Blocks] -> CRC Append -> [ Block Bytes ] -> Channel Encoder -> Costas Array Surround -> Modulator
                    |         ^                                                                                                                      ^
                    v        /                                                                                                                       |
                   CQ Detector                                                                                                                 UI TX Offset

 Q. Does the Source Encoder add the CRC, or is it a separate step?

 Receiver:
 Audio In -> Fourier Transform -> [Spectrum] -> Costas Array Detector -> Channel Detectors

 The Costas Array Detector finds Costas Arrays in the Spectrum stream, and allocates a Channel Detector at the Audio
 Offset it finds the array. Each Channel Detector receives the Spectrum stream, which it passes through a Narrow Band
 Pass Filter to see if it has any data - if so it demodulates it, etc...

 Channel Detectors time out and are destroyed after 1 minute (configurable) of not receiving anything. This removes the
 metadata they have received from the Waterfall display.


 Channel Detector: [Spectrum, Audio Offset of Costas Array] -> Narrow Band Pass Filter -> Demodulator -> Channel Decoder -> [Source Encoded Block, Erasures?] -> CRC Filter ->
   -> Source Decoder -> [Metadata, Audio Offset, Keying Events] -> Receiver UI

 Receiver UI: [Metadata, Audio Offset, Keying Events] -> Current User Band Pass Filter -> Merged Sound Output -> Audio Out
                                  |
                                  |
                                  v
                      Waterfall metadata display




 Source Encoder Frames and Block Size
 ------------------------------------
 The source encoder receives a stream of
 keying data, and must encode this into a compressed stream of encoded frames, which are concatenated
 to form a block, which has a fixed size. If an incoming encoded frame would overflow the size of the
 current block, the block is instead padded out to its maximum size, and emitted. The incoming encoded
 frame then becomes the first frame of the new block. (It is pushed onto a stack of frames to emit)

 Not holding back a block if the user pauses keying
 --------------------------------------------------
 If the stream of keying events 'dries up' without a keying end event, the block should be padded
 and emitted.
 This would also happen on receipt of a keying end event.

 Adding metadata into the source encoder frame stream
 ----------------------------------------------------
 The keying stream would be split and connected to:
 * a CQ detector (whose output is connected to the source encoder)
 * the source encoder
 The source encoder receives the keying stream, but when CQ is detected, it resets a state machine
 that includes the callsign metadata as a source encoder frame in the current block (by pushing it
 onto the stack of frames to emit). [this stack mechanism might cause the next metadata frame, the
 location, to be added to the same frame as the callsign]. the state machine will queue up the three
 metadata frames (callsign, locator, power) to be encoded in the next three blocks. If CQ is asserted
 when the state machine is not idle, it is ignored - if the user sends many CQs that overflow a block
 then the first block emitted would contain the callsign and some CQ keying; the second block would
 include the location and some CQ keying; the third block would include the power and some CQ keying.
 If they're still CQing when the state machine goes idle, the fourth block could include the callsign
 again.

 The source encoder also has a timer that queues the callsign metadata periodically even if there is
 no CQ detected; this is to comply with license regulations that state stations must periodically
 identify themselves. This does not trigger the three frames, just the callsign; a block will only
 contain one instance of the three frames: it could not contain two callsigns for example.

 If the user isn't keying, but leaves the station alone for a while, should this timer automatically
 generate a single callsign metadata frame, padded to the block size, and emit it for channel
 encoding and transmission? Or should this only happen during active operation?

 Additional metadata frames
 --------------------------
 Every block contains a WPM and start-of-keying polarity frame, emitted prior to the first keying
 event.

 Cyclic Redundancy Check
 -----------------------
 Upon emitting a block, a suitable CRC is appended. The strength of this would be chosen to be
 efficient but capable of detecting a decent proportion of decode errors. What is 'good enough'?
 What are the trade-offs? To be investigated.

 Source Encoder Frame Types
 --------------------------
 1. WPM/Polarity - add on first keying event of a new block.
 2. Callsign
 3. Location
 4. Power
 5. Keying - is start / stop needed?
 6. Padding
